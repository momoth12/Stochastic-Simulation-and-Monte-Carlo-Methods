{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP556, Ecole Polytechnique, 2023-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 - Variance Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a model defined by the relationship $Y = f(X)$.\n",
    "The objective is to estimate the value $\\mathbb{E}\\bigl[g(Y)\\bigl]$, for a certain function $g$ over the set of outputs $Y$. We assume that $g(Y)$ is square-integrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. Control Variables\n",
    "Black-box models representing input-output systems of numerical codes are often computationally expensive. Let's suppose we have a reduced input-output model $Y_r = f_r(X)$ that is easier to simulate for different input values $X$ than the model $Y = f(X)$, and so that we know the quantity $m_r = \\mathbb{E}\\bigl[g(Y_r)\\bigl]$. Furthermore, we assume $\\mathbb{E}\\bigl[g(Y_r)^2\\bigl]<\\infty$.\n",
    "\n",
    "We denote $(X_i)_{1\\leq i\\leq n}$ as a sequence of independent copies of the input variable $X$, and we define\n",
    "\\begin{eqnarray*}\n",
    " I_n&=&\\frac{1}{n}\\sum_{i=1}^n g(f(X_i)),\\qquad\n",
    " I_n^c = m_r+\\frac{1}{n}\\sum_{i=1}^n \\bigl( g(f(X_i)) - g(f_r(X_i)) \\bigr) .\n",
    " \\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 1:\n",
    "Verify that $I_n$ and $I_n^c$ are unbiased estimators of $\\mathbb{E}\\bigl[g(Y)\\bigl]$, and calculate their variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Assuming that the input variables $X$ follow a uniform distribution on $[0,1]$, $f(x)=e^x$, $f_r(x)=1+x$, and $g(y)=y$.\n",
    "\n",
    "Simulate both estimators and their asymptotic 95% confidence intervals.\n",
    "\n",
    "Plot the curves of the empirical means $I_n$ and $I_n^c$.\n",
    "\n",
    "What is the gain in terms of the number of simulations for the estimator $I_n^c$ to achieve the same asymptotic precision as the naive Monte Carlo method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 # Sample size\n",
    "\n",
    "integers1toN = np.arange(1,N+1) # A vector containing integers from 1 to N\n",
    "\n",
    "############################################\n",
    "# Calculate the exact value of m_r\n",
    "#m_r = ????\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Simulate N samples of values of Y and Y_control\n",
    "#Y = ????\n",
    "\n",
    "#Y_control = ????\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Calculate both estimators and their \n",
    "# empirical variances\n",
    "############################################\n",
    "#Mean_MC = ????\n",
    "#Variance_MC = ????\n",
    "\n",
    "#Mean_control = ????\n",
    "#Variance_control = ????\n",
    "\n",
    "#halfWidthCI95_MC = ????\n",
    "#halfWidthCI95_Control = ????\n",
    "\n",
    "############################################\n",
    "# Gain in terms of number of simulations\n",
    "# for the same precision\n",
    "\n",
    "#control_gain = ????\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# For display: exact value of E[g(Y)]\n",
    "Esp_gY = np.exp(1.)-1.\n",
    "\n",
    "print(\"Sample size = %d\" %N)\n",
    "print(\"Esp_gY = %1.3f \\n\" %Esp_gY)\n",
    "print(\"MC Estimator: mean = %1.3f emp variance = %1.3f\"\n",
    "%(Mean_MC, Variance_MC))\n",
    "print(\"CI(95%%) = [%1.3f,%1.3f] \\n\"\n",
    "%(Mean_MC-halfWidthCI95_MC, Mean_MC+halfWidthCI95_MC))\n",
    "\n",
    "print(\"Control Estimator: mean = %1.3f empirical variance = %1.3f\"\n",
    "%(Mean_control, Variance_control))\n",
    "print(\"CI(95%%) = [%1.3f,%1.3f] \\n\"\n",
    "%(Mean_control-halfWidthCI95_Control, Mean_control+halfWidthCI95_Control))\n",
    "\n",
    "print(\"Gain in number of simulations with control variable: %1.2f\" %control_gain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "## Displaying 10 trajectories \n",
    "############################################\n",
    "\n",
    "# Simulating 10 trajectories of the estimators\n",
    "# I_n and I_n^c\n",
    "# Expected size of the arrays: M x N\n",
    "M = 10\n",
    "\n",
    "#Y = ????\n",
    "#Ycontrol = ????\n",
    "#I_n = ????\n",
    "#Ic_n = ????\n",
    "\n",
    "## Display the 10 trajectories\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(integers1toN, I_n[0], color=\"b\", label=\"MC\")\n",
    "ax.plot(integers1toN, I_n[1:].T, color=\"b\")\n",
    "\n",
    "ax.plot(integers1toN, Ic_n[0], color=\"g\", label=\"Control\")\n",
    "ax.plot(integers1toN, Ic_n[1:].T, color=\"g\")\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_ylim(1.4, 2.2)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "Let $(I^j_n)_{1\\leq i\\leq M}$ and $(I^{c,j}_n)_{1\\leq i\\leq M}$ be the empirical estimations associated with $M$ independent draws of the two estimators.\n",
    "\n",
    "Explicitly evaluate $m = \\mathbb{E}\\bigl[g(Y)\\bigl]$ and plot histograms of the errors $(I^j_n - m)_{1 \\le j \\le M}$ and $(I^{c,j}_n - m)_{1 \\le j \\le M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 500 # Sample size\n",
    "M = 1000 # Number of estimator draws\n",
    "\n",
    "integers1toN = np.arange(1,N+1) # A vector containing integers from 1 to N\n",
    "\n",
    "Esp_gY = np.exp(1.)-1.\n",
    "m_r = 1.5\n",
    "\n",
    "###############\n",
    "# Simulate the samples of values of Y \n",
    "# and Y_control\n",
    "\n",
    "#Y = ????\n",
    "#Ycontrol = ????\n",
    "###############\n",
    "\n",
    "############################################\n",
    "# Samples of size M for both estimators\n",
    "\n",
    "#I_N = ????\n",
    "#Ic_N = ????\n",
    "\n",
    "############################################\n",
    "## Displaying histograms of errors\n",
    "## for both estimators\n",
    "\n",
    "#plt.hist(????, density=\"True\", bins=int(np.sqrt(M)), label=\"MC\")\n",
    "#plt.hist(????, density=\"True\", bins=int(np.sqrt(M)), label=\"Control\")\n",
    "plt.title(\"Estimation with control variable, N = %1.0f\" %N)\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Optimal Control Variable\n",
    "\n",
    "Now, let's consider the estimator\n",
    "\\begin{eqnarray*}\n",
    "I_n^\\lambda = \\lambda  m_r + \\frac{1}{n}\\sum_{i=1}^n \\bigl(g(f(X_i)) - \\lambda  g(f_r(X_i))\\bigr),\n",
    "\\qquad \\lambda \\in \\mathbb R.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Suggest a choice for the parameter $\\lambda$.\n",
    "\n",
    "Plot the trajectories of the empirical means $I_n^\\lambda$ obtained with this choice, and then plot the histogram of the errors of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Estimating optimal lambda from a small\n",
    "## number of simulations\n",
    "############################################\n",
    "n = 100\n",
    "X = np.random.rand(n)\n",
    "Y = np.exp(X)\n",
    "\n",
    "## Empirical optimal lambda\n",
    "#lambda_opt = ????\n",
    "\n",
    "print(\"Optimal lambda = %1.3f\" %lambda_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### For histograms: M x N draws\n",
    "#############################################\n",
    "M = 1000\n",
    "N = 500\n",
    "integers1toN = np.arange(1,N+1)\n",
    "\n",
    "Y = ????\n",
    "Ycontrol = ????\n",
    "\n",
    "#############################################\n",
    "### Displaying the first 10 trajectories\n",
    "#############################################\n",
    "\n",
    "I_n = np.cumsum(Y[0:10,:], axis=1) / integers1toN\n",
    "Ic_n = np.cumsum( ????[0:10,:], axis=1) / integers1toN\n",
    "\n",
    "## Display the first 10 trajectories\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(integers1toN, I_n[0], color=\"b\", label=\"MC\")\n",
    "ax.plot(integers1toN, I_n[1:].T, color=\"b\")\n",
    "\n",
    "ax.plot(integers1toN, Ic_n[0], color=\"g\", label=\"Control\")\n",
    "ax.plot(integers1toN, Ic_n[1:].T, color=\"g\")\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_ylim(1.4, 2.2)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "# Calculate both estimators I_N and I_N control and their\n",
    "# empirical variances\n",
    "Mean_MC = np.mean( ???? )\n",
    "Variance_MC = np.var( ???? )\n",
    "\n",
    "Mean_control = np.mean( ???? )\n",
    "Variance_control = np.var( ???? )\n",
    "\n",
    "halfWidthCI95_MC = np.sqrt(Variance_MC / N)*1.96\n",
    "halfWidthCI95_Control = np.sqrt(Variance_control / N)*1.96\n",
    "\n",
    "control_gain = Variance_MC/Variance_control\n",
    "\n",
    "print(\"Sample size = %d\" %N)\n",
    "print(\"Esp_gY = %1.3f \\n\" %Esp_gY)\n",
    "print(\"MC Estimator: mean = %1.3f  empirical variance = %1.3f\" \\\n",
    "      %(Mean_MC, Variance_MC))\n",
    "print(\"CI(95%%) = [%1.3f,%1.3f] \\n\" \\\n",
    "      %(Mean_MC-halfWidthCI95_MC, Mean_MC+halfWidthCI95_MC))\n",
    "\n",
    "print(\"Control Estimator: mean = %1.4f  empirical variance = %1.3f\" \\\n",
    "      %(Mean_control, Variance_control))\n",
    "print(\"CI(95%%) = [%1.3f,%1.3f] \\n\" \\\n",
    "      %(Mean_control-halfWidthCI95_Control, Mean_control+halfWidthCI95_Control))\n",
    "\n",
    "print(\"Gain in number of simulations with control variable: %1.2f\" %control_gain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### For the histogramms: evaluate the \n",
    "## errors for both estimators I_N and I_N control\n",
    "#############################################\n",
    "\n",
    "error_N = ????\n",
    "error_control_N = ????\n",
    "\n",
    "plt.hist(error_N, density=\"True\", bins=int(np.sqrt(M)), label=\"MC\")\n",
    "\n",
    "plt.hist(error_control_N, density=\"True\", bins=int(np.sqrt(M)), label=\"Control\")\n",
    "\n",
    "plt.title(\"Estimation with control variable, N = %1.0f\" %N)\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Antithetic Sampling and Stratification Methods\n",
    "We assume that the input variables $X$ follow a uniform distribution between $-1$ and $+1$. We are in the situation where $f(x)=e^x$, and $g(y)=y$, and we want to estimate $\\mathbb{E}\\bigl[g(Y)\\bigl]= \\mathbb{E}\\bigl[e^X\\bigl]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Basic Monte Carlo Method\n",
    "\n",
    "Let $I_n$ be the empirical mean of $n$ independent copies of $X$.\n",
    "\n",
    "Verify that we have $\\mathbb{E}\\bigl[e^X\\bigl] = \\sinh(1)\\simeq 1.18$, and $\\mbox{Var}(e^X)= \\frac{1}{2} (1-e^{-2})\\simeq 0.43$.\n",
    "\n",
    "Plot the curves of the empirical means obtained from multiple realizations and compare them with the desired quantity.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to be used for the rest of the exercise\n",
    "\n",
    "N = 1000 # Sample size\n",
    "integers1toN = np.arange(1,N+1) # A vector containing integers from 1 to N\n",
    "\n",
    "Esp_gY = np.sinh(1.0)\n",
    "Var_gY = (1.0 - np.exp(-2.))/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Complete with N draws from the uniform distribution [-1,1]\n",
    "# and draws of Y = exp(X)\n",
    "X = ????\n",
    "Y = ????\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Store in 'mean' the MC estimation of E[g(Y)],\n",
    "# in 'var' the empirical variance, and in 'halfWidthCI'\n",
    "# the half width of the asymptotic 95% confidence interval for E[g(Y)]\n",
    "mean = ????\n",
    "var = ????\n",
    "halfWidthCI = ????\n",
    "############################################\n",
    "\n",
    "print(\"MC Estimator \\n\")\n",
    "\n",
    "print(\"Esp_gY = %1.3f Var_gY = %1.3f\" %(Esp_gY, Var_gY))\n",
    "print(\"mean = %1.3f  var = %1.3f\" %(mean,var))\n",
    "print(\"95%% confidence interval for E[g(Y)] = [ %1.3f , %1.3f ] \\n\" %(mean - halfWidthCI, mean + halfWidthCI))\n",
    "print(\"Relative error = %1.3f\" %(halfWidthCI/mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # number of realizations of empirical means\n",
    "\n",
    "############################################\n",
    "# Complete with N draws from the uniform distribution [-1,1]\n",
    "# and antithetic draws\n",
    "X = ????\n",
    "Y = ????\n",
    "I_n =  ????\n",
    "############################################\n",
    "\n",
    "# Displaying trajectories\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(integers1toN, I_n.T, color=\"b\")\n",
    "\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_ylim(1.0, 1.3)\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Antithetic Sampling\n",
    "\n",
    "Verify that $(f(u)-f(v))(f(-u)-f(-v))\\leq 0$, for all $(u,v)\\in [-1,1]$, and deduce a antithetic sampling technique based on simulating $n$ uniform variables on $[-1,1]$.\n",
    "\n",
    "Let $I^{\\prime}_n$ be the corresponding empirical estimator.\n",
    "\n",
    "Plot the curves of the empirical means obtained.\n",
    "\n",
    "What is the gain in terms of the number of simulations for this estimator, for the same asymptotic accuracy as the naive Monte Carlo method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Complete with N draws from the uniform distribution [-1,1]\n",
    "# and antithetic draws\n",
    "X = ????\n",
    "Z = ????\n",
    "#\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Complete with the calculation of the antithetic estimator\n",
    "mean = ????\n",
    "var = ????\n",
    "halfWidthCI = ????\n",
    "############################################\n",
    "\n",
    "print(\"Antithetic Estimator \\n\")\n",
    "\n",
    "print(\"Esp_gY = %1.3f\" %(Esp_gY))\n",
    "print(\"I_prime_mean = %1.3f  empirical variance = %1.3f\" %(mean, var))\n",
    "print(\"95%% Confidence Interval for E[g(Y)] = [ %1.3f , %1.3f ] \\n\" %(mean - halfWidthCI, mean + halfWidthCI))\n",
    "print(\"Relative error = %1.3f\" %(halfWidthCI/mean))\n",
    "print(\"gain in number of simulations compared to MC: %1.2f\" %(Var_gY/var))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Empirical mean trajectories\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Evaluate M trajectories of the empirical estimator I'_n\n",
    "X = ????\n",
    "Z = ????\n",
    "\n",
    "############################################\n",
    "# Complete with the calculation of\n",
    "# antithetic estimator trajectories\n",
    "I_prime_n = ????\n",
    "############################################\n",
    "\n",
    "# Displaying the estimator trajectories\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(integers1toN, I_prime_n.T, color=\"b\")\n",
    "\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_ylim(1.0, 1.3)\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Proportional Stratification\n",
    "Propose a proportional stratification technique based on simulating uniform variables on $[-1,0]$ and uniform variables on $[0,1]$.\n",
    "\n",
    "Let $J_n$ be the corresponding estimator. Plot the estimator trajectories as a function of $n$. Does this estimator satisfy the Central Limit Theorem (CLT)? What is the gain in terms of the number of simulations for this estimator, for the same asymptotic accuracy as the naive Monte Carlo method?\n",
    "\n",
    "Can we explicitly calculate $\\mbox{Var}(J_n)$? (see question 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportionnal stratification (we will take N even)\n",
    "N1 = int(N/2)\n",
    "N2 = N1\n",
    "\n",
    "############################################\n",
    "# Complete with N1 and N2 draws of conditional distributions\n",
    "# on strata\n",
    "X1 = ????\n",
    "X2 = ????\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "# Stratified estimator\n",
    "J_N = ????\n",
    "\n",
    "############################################\n",
    "# We evaluate the asymptotic variance through simulation\n",
    "# of the estimator J_N\n",
    "############################################\n",
    "\n",
    "var = ????\n",
    "\n",
    "halfWidthCI = 1.96 * np.sqrt(var / N)  # because J_N satisfies a CLT\n",
    "relativeError = halfWidthCI / J_N\n",
    "\n",
    "#######\n",
    "print(\"Estimator by proportionnal stratification \\n\")\n",
    "\n",
    "print(\"E[g(Y)] = %1.3f Var[g(Y)] = %1.3f\" %(Esp_gY, Var_gY))\n",
    "print(\"J_N = %1.3f Estimated Variance  = %1.3f\" %(J_N, var))\n",
    "print(\"CI = [%1.3f,%1.3f] \\n\" %(J_N - halfWidthCI, J_N + halfWidthCI))\n",
    "print(\"Relative Error = %1.3f\" %relativeError)\n",
    "\n",
    "print(\"Savings in the number of simulations compared to MC: %1.2f\" %(Var_gY/var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Trajectories of the stratified estimator J_n\n",
    "## for n = 2, 4, ..., N\n",
    "############################################\n",
    "M = int(10)\n",
    "\n",
    "############################################\n",
    "# Complete with M x N1 and M x N2 draw\n",
    "# of the conditionnal laws on strata\n",
    "X1 = ?????\n",
    "X2 = ?????\n",
    "############################################\n",
    "\n",
    "J_n = np.zeros((M, int(N/2)))\n",
    "\n",
    "############################################\n",
    "# Complete with the calculus of the stratified\n",
    "# estimator for n@ 2,4,...,N\n",
    "\n",
    "for n in np.arange(2, N+2, 2):\n",
    "    n1 = n // 2\n",
    "    J_n[:, n1-1] = ????\n",
    "\n",
    "############################################\n",
    "# Displaying the trajectories of the stratified estimator\n",
    "## for n = 2, 4, ..., N\n",
    "\n",
    "evenIntegers1toN = np.arange(2, N+2, 2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(evenIntegers1toN, J_n[1:10, :].T, color=\"b\")\n",
    "\n",
    "ax.set_ylim(1.0, 1.3)\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Non-Proportional Stratification (Optional)\n",
    "\n",
    "Propose a stratification technique based on simulating a proportion $n_1=rn$ of uniform variables on $[-1,0]$, and $n_2=(1-r)n$ of uniform variables on $[0,1]$, with $r \\in (0,1)$ such that $n_1$ and $n_2$ are integers.\n",
    "\n",
    "Plot the trajectories of these estimators as a function of $n$ for various values of $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Proportional Stratification\n",
    "\n",
    "def nonProportionalStratification(r):\n",
    "    N1 = int(N * r)\n",
    "    N2 = int(N - N1)\n",
    "    \n",
    "    ################\n",
    "    # Complete with N1 and N2 draws\n",
    "    # of conditional distributions on strata\n",
    "    X1 = ????\n",
    "    X2 = ????\n",
    "    ################\n",
    "    \n",
    "    ################\n",
    "    # Stratified estimator\n",
    "    J_N = ????\n",
    "    \n",
    "    ############################################################\n",
    "    # Evaluate the variance of the estimator J_N through simulation\n",
    "    var = ????\n",
    "    \n",
    "    halfWidthCI = 1.96 * np.sqrt(var)\n",
    "    \n",
    "    relativeError = halfWidthCI / J_N\n",
    "    \n",
    "    print(\"Non-Proportional Stratification Estimator, r = %1.2f \\n\" %r)\n",
    "    \n",
    "    print(\"E[g(Y)] = %1.4f Var[g(Y)] = %1.4f\" %(Esp_gY, Var_gY))\n",
    "    print(\"J_N = %1.4f  var*N = %1.4f\" %(J_N, var))\n",
    "    print(\"CI = [%1.4f,%1.4f] \\n\" %(J_N - halfWidthCI, J_N + halfWidthCI))\n",
    "    \n",
    "    print(\"Relative Error = %1.4f\" %relativeError)\n",
    "    print(\"Savings in the number of simulations compared to MC : %1.4f\" %(Var_gY/var))\n",
    "    \n",
    "    ############################################\n",
    "    ## Trajectories of the stratified estimator for n=1,..,N\n",
    "    ############################################\n",
    "    M = 10\n",
    "    \n",
    "    #####################\n",
    "    # Complete with M x N1 and M x N2 draws\n",
    "    # of conditionnal laws on strata\n",
    "    #X1 = ?????\n",
    "    #X2 = ?????\n",
    "    \n",
    "    Y1 = np.exp(X1); Y2 = np.exp(X2)\n",
    "    \n",
    "    J_n = np.zeros((M, N))\n",
    "    \n",
    "    #####################\n",
    "    # Complete with the calculus of the stratified\n",
    "    # estimator for different values of n\n",
    "    for n in np.arange(int(1/r), N):\n",
    "        n1 = int(n * r)\n",
    "        n2 = n - n1\n",
    "        J_n[:, n] = ????\n",
    "        \n",
    "    ############################################\n",
    "    # Display the trajectories\n",
    "    integers1toN = np.arange(1, N+1)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(integers1toN, J_n[1:10, :].T, color=\"b\")\n",
    "    \n",
    "    ax.set_ylim(1.0, 1.3)\n",
    "    ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "#nonProportionalStratification(0.8)\n",
    "#nonProportionalStratification(0.4)\n",
    "#nonProportionalStratification(0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 5: Optimal stratification on both strata$(S_1,S_2)=([-1,0],[0,1])$ (Optional)\n",
    "\n",
    "Calculate the values of the parameters\n",
    "\\begin{eqnarray*}\n",
    "\\sigma_1^2&:=&\\mathbb{E}\\bigl[e^{2X}~|~X\\in[-1,0]\\bigl] - \\mathbb{E}\\bigl[e^{X}~|~X\\in[-1,0]\\bigl]^2\\\\\n",
    "\\sigma_2^2&:=&\\mathbb{E}\\bigl[e^{2X}~|~X\\in[0,1]\\bigl] - \\mathbb{E}\\bigl[e^{X}~|~X\\in[0,1]\\bigl]^2\n",
    "\\end{eqnarray*}\n",
    "and propose an optimal stratification technique.\n",
    "Simulate the empirical estomator associated and plot its trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explicit Calculation of sigma_1 and sigma_2\n",
    "sigma_1 = np.sqrt(0.5 * (1. - np.exp(-2.)) - (1. - np.exp(-1.))**2)\n",
    "sigma_2 = np.sqrt(0.5 * (np.exp(2.) - 1.) - (np.exp(1.) - 1.)**2)\n",
    "\n",
    "############################################\n",
    "## Compute the optimal N1\n",
    "#r = ????\n",
    "\n",
    "N1 = int(N * r)\n",
    "N2 = int(N - N1)\n",
    "\n",
    "############################################\n",
    "## Simulation of the non-proportional \n",
    "## stratification estimator with optimal N1\n",
    "##\n",
    "## This is the same code as in Question 4: we could directly call\n",
    "## the nonProportionalStratification(r) function\n",
    "## The only difference in the code below is that we use\n",
    "## the explicit variance of the stratified estimator instead of estimating it.\n",
    "#X1 = ????\n",
    "#X2 = ????\n",
    "\n",
    "############################################\n",
    "# Stratified estimator\n",
    "\n",
    "#J_N = ????\n",
    "\n",
    "############################################\n",
    "## Compute the variance of the optimal J_N estimator\n",
    "## We can use the explicit expression of the variance of J_N\n",
    "#var = ?????\n",
    "\n",
    "\n",
    "halfWidthCI = 1.96 * np.sqrt(theoretical_variance / N)\n",
    "relativeError = halfWidthCI / J_N\n",
    "\n",
    "############################################\n",
    "print(\"Estimator by optimal non-proportional stratification, r_optimal = %1.3f\" % r)\n",
    "\n",
    "print(\"E[g(Y)] = %1.3f Var[g(Y)] = %1.3f\" % (Esp_gY, Var_gY))\n",
    "print(\"J_mean = %1.3f  theoretical_variance = %1.3f\" % (J_N, var))\n",
    "print(\"CI = [%1.3f,%1.3f] \\n\" % (J_N - halfWidthCI, J_N + halfWidthCI))\n",
    "\n",
    "print(\"Relative Error = %1.3f\" % relativeError)\n",
    "print(\"Savings in the number of simulations compared to MC: %1.2f\" % (Var_gY / var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Complete the trajectories of the stratified estimator\n",
    "# for n = 1, .., N\n",
    "J_n = np.zeros((M, N))\n",
    "\n",
    "for n in np.arange(int(1 / r), N):\n",
    "    n1 = int(n * r)\n",
    "    n2 = n - n1\n",
    "    J_n[:, n] = ????\n",
    "\n",
    "############################################\n",
    "# Displaying the trajectories\n",
    "integers1toN = np.arange(1, N + 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(integers1toN, J_n[1:10, :].T, color=\"b\")\n",
    "\n",
    "ax.set_ylim(1.0, 1.3)\n",
    "ax.axhline(Esp_gY, color=\"r\", label=\"Expectation\")\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
